<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emotion Detection</title>
    <style>
        body {
            margin: 0;
            padding: 0;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background: url('../Bilder_InsideOut/1341737.jpeg') no-repeat center center fixed;
            background-color: #f0f0f0;
            font-family: Arial, sans-serif;
        }
        #container {
            display: flex;
            flex-direction: column;
            align-items: center;
            max-width: 800px;
            margin-top: 20px;
        }
        #videoContainer {
            position: relative;
            width: 480px;
            height: 320px;
            background-color: #000;
            margin-bottom: 20px;
        }
        #video {
            width: 100%;
            height: 100%;
            border-radius: 8px;
        }
        canvas {
            position: absolute;
            top: 0;
            left: 0;
            border-radius: 8px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
            background-color: #007BFF;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        button:hover {
            background-color: #0056b3;
        }
        #explanation {
            width: 100%;
            max-width: 600px;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
            text-align: left;
            line-height: 1.5;
        }
    </style>
</head>
<body>
    <div id="container">
        <div id="videoContainer">
            <video id="video" width="480" height="320" controls>
                <source src="../Videos/Joy.mp4" type="video/mp4">
                <source src="../Videos/Sadness.mp4" type="video/mp4">
                <source src="../Videos/Angry.mp4" type="video/mp4">
                <source src="../Videos/Disgust.mp4" type="video/mp4">
                <source src="../Videos/Fear.mp4" type="video/mp4">
                <source src="../Videos/neutral.mp4" type="video/mp4">
                Dein Browser unterstützt das Video-Tag nicht.
            </video>
        </div>
        <button id="startEmojiButton">Start Emotion Detection</button>
        <div id="explanation">
            <h2>Funktion der Anwendung:</h2>
            <p>Die Reise beginnt mit einer Gesichtserkennung und Emotionserkennung. In dieser Anwendung wirst du von den Emotionen-Charakteren aus "Inside Out" begleitet. Halte dein Gesicht vor die Kamera, zeige deine aktuelle Stimmung und Emotionen, und drücke den Button „Anzeigen“. Basierend auf deiner Stimmung erhältst du eine personalisierte Rückmeldung von einem der Charaktere, wie z.B. Freude könnte dir ermutigende und fröhliche Nachrichten schicken.</p>
        </div>
    </div>
    <script src="face-api.min.js"></script>
    <script>
        const video = document.getElementById('video');
        const startEmojiButton = document.getElementById('startEmojiButton');

        console.log("Loading models...");
        Promise.all([
            faceapi.nets.tinyFaceDetector.loadFromUri('../models/tiny_face_detector_model-weights_manifest.json'),
            faceapi.nets.faceLandmark68Net.loadFromUri('../models/face_landmark_68_model-weights_manifest.json'),
            faceapi.nets.faceRecognitionNet.loadFromUri('../models/face_recognition_model-weights_manifest.json'),
            faceapi.nets.faceExpressionNet.loadFromUri('../models/face_expression_model-weights_manifest.json')
        ]).then(() => {
            console.log("Models loaded");
            startVideo();
        }).catch(err => console.error("Error loading models: ", err));

        function startVideo() {
            console.log("Starting video...");
            navigator.mediaDevices.getUserMedia({ video: {} })
                .then(stream => {
                    video.srcObject = stream;
                    console.log("Video stream started");
                })
                .catch(err => console.error("Error starting video stream: ", err));
        }

        video.addEventListener('play', () => {
            console.log("Video is playing");
            const canvas = faceapi.createCanvasFromMedia(video);
            document.getElementById('videoContainer').append(canvas);
            const displaySize = { width: video.width, height: video.height };
            faceapi.matchDimensions(canvas, displaySize);

            setInterval(async () => {
                try {
                    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();
                    const resizedDetections = faceapi.resizeResults(detections, displaySize);
                    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                    faceapi.draw.drawDetections(canvas, resizedDetections);
                    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
                    faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
                } catch (error) {
                    console.error("Error detecting faces: ", error);
                }
            }, 1000);
        });

        function getVideoForExpression(expressions) {
            const expressionThreshold = 0.1;
            if (expressions.happy > expressionThreshold) return '../Videos/Joy.mp4';
            if (expressions.sad > expressionThreshold) return '../Videos/Sadness.mp4';
            if (expressions.angry > expressionThreshold) return '../Videos/Angry.mp4';
            if (expressions.disgusted > expressionThreshold) return '../Videos/Disgust.mp4';
            if (expressions.fearful > expressionThreshold) return '../Videos/Fear.mp4';
            return '../Videos/neutral.mp4';
        }

        async function detectExpressionAndRedirect() {
            console.log("Detecting expressions...");
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();
            if (detections.length > 0) {
                const currentExpressions = detections[0].expressions;
                const videoSrc = getVideoForExpression(currentExpressions);
                console.log("Redirecting to: ", videoSrc);
                window.location.href = `videoPage.html?video=${encodeURIComponent(videoSrc)}`;
            } else {
                console.log("No faces detected");
            }
        }

        startEmojiButton.addEventListener('click', () => {
            console.log("Start button clicked");
            detectExpressionAndRedirect();
        });

        // Initial video start
        startVideo();
    </script>
</body>
</html>
